{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LXhyDGGk_iEf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696177169567,"user_tz":-420,"elapsed":29693,"user":{"displayName":"Lưu Gia Huy","userId":"01458389295153929324"}},"outputId":"e8effd70-9019-486c-938a-ab4b175fecb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting esprima\n","  Downloading esprima-4.0.1.tar.gz (47 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: esprima\n","  Building wheel for esprima (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for esprima: filename=esprima-4.0.1-py3-none-any.whl size=62240 sha256=77e65280a87ea099d7e7390bbe943f7e6e1f9e290be8986d72c650348c5f71a4\n","  Stored in directory: /root/.cache/pip/wheels/7c/ad/8b/afd6e521e6aaea5482b7b4665ff3ce5a92373bd285e7d3a85c\n","Successfully built esprima\n","Installing collected packages: esprima\n","Successfully installed esprima-4.0.1\n","Mounted at /content/drive\n"]}],"source":["!pip install esprima\n","import urllib.parse\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","from collections import defaultdict\n","import html5lib\n","import esprima\n","import requests\n","import csv\n","import zipfile\n","import io\n","from bs4 import BeautifulSoup\n","import logging\n","from collections import deque\n","import pandas as pd\n","import numpy as np\n","import urllib.parse\n","import re\n","from collections import defaultdict\n","import html5lib\n","import esprima\n","from urllib import parse as urlparse\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5rd2Wfx_mBV"},"outputs":[],"source":["def parse_html(web_page):\n","    return BeautifulSoup(web_page, 'html5lib')\n","\n","def create_html_feature_vector(page_content):\n","    TG = [\"main\", \"section\", \"script\", \"iframe\", \"meta\", \"applet\", \"object\", \"embed\", \"link\", \"svg\", \"frame\", \"form\", \"div\", \"style\", \"video\", \"img\", \"input\", \"textarea\"]\n","    AT = [\"selected\", \"disabled\", \"target\", \"class\", \"action\", \"archive\", \"background\", \"cite\", \"classid\", \"codebase\", \"data\", \"dsync\", \"formaction\", \"href\", \"icon\", \"longdesc\", \"manifest\", \"poster\", \"profile\", \"src\", \"usemap\", \"http-equiv\", \"lowsrc\"]\n","    EV = [\"abort\", \"activate\", \"afterprint\", \"afterupdate\", \"beforeactivate\", \"beforecopy\", \"beforecut\", \"beforedeactivate\", \"beforeeditfocus\", \"beforepaste\", \"beforeprint\", \"beforeunload\", \"blur\", \"change\", \"click\", \"contextmenu\", \"copy\", \"cut\", \"datasetcomplete\", \"dblclick\", \"deactivate\", \"drag\", \"dragend\", \"dragenter\", \"dragleave\", \"dragover\", \"dragstart\", \"drop\", \"error\", \"focus\", \"focusin\", \"focusout\", \"hashchange\", \"help\", \"input\", \"keydown\", \"keypress\", \"keyup\", \"load\", \"mousedown\", \"mouseenter\", \"mouseleave\", \"mousemove\", \"mouseout\", \"mouseover\", \"mouseup\", \"mousewheel\", \"paste\", \"propertychange\", \"readystatechange\", \"reset\", \"resize\", \"resizestart\", \"scroll\", \"search\", \"select\", \"selectstart\", \"start\", \"submit\", \"unload\"]\n","    html_keyword_evil = []\n","\n","    HFV = defaultdict(int)\n","\n","    P = parse_html(page_content)\n","\n","    for node in P.find_all():\n","        for ti in TG:\n","            HFV[\"html_tag_\" + ti] += len(node.find_all(ti))\n","\n","        for ai in AT:\n","            HFV[\"html_attr_\" + ai] += len(node.attrs.get(ai, []))\n","\n","        for ei in EV:\n","            HFV[\"html_event_on\" + ei] += len(node.attrs.get(\"on\" + ei, []))\n","\n","    HFV['hl'] = len(page_content)\n","    return HFV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwx2q2hZ_ogF"},"outputs":[],"source":["def parse_url_address(page_url):\n","    AT = [\"selected\", \"disabled\", \"target\", \"class\", \"action\", \"archive\", \"background\", \"cite\", \"classid\", \"codebase\", \"data\", \"dsync\", \"formaction\", \"href\", \"icon\", \"longdesc\", \"manifest\", \"poster\", \"profile\", \"src\", \"usemap\", \"http-equiv\", \"lowsrc\"]\n","    TG = [\"script\", \"iframe\", \"meta\", \"applet\", \"object\", \"embed\", \"link\", \"svg\", \"frame\", \"form\", \"div\", \"style\", \"video\", \"img\", \"input\", \"textarea\", \"table\", \"footer\",\"main\", \"section\", \"article\", \"aside\"]\n","    EV = [\"onblur\", \"onclick\", \"onerror\", \"onfocus\", \"onload\", \"onmousemove\", \"onmouseout\", \"onmouseover\", \"onsearch\", \"onsubmit\", \"onunload\", \"ondblclick\", \"onscroll\", \"oninput\"]\n","\n","\n","    url_redirections = ['document.URL',\n","    'document.URLUnencoded',\n","    'document.baseURI',\n","    'document.documentURI',\n","    'location',\n","    'window.location',\n","    'window.history',\n","    'window.navigate',\n","    'window.open',\n","    'self.location',\n","    'top.location']\n","    url_number_keywords_param = ['search','login', 'signup', 'query', 'contact', 'URL', 'redirect']\n","    url_number_keywords_evil = [\"<\",\">\",\"javascript\", \"alert\", \"script\", \"onerror\" ,\"iframe\" ,\"cookie\", \"sCrIpT\", \"marquee\", \"fromCharCode\"]\n","\n","    UFV = {}\n","\n","\n","    url_str = urllib.parse.unquote(page_url)\n","    UFV['url_length'] = len(url_str)\n","\n","    UFV['url_duplicated_characters'] = int(any(url_str.count(char) > 1 for char in set(url_str)))\n","\n","\n","    special_characters = set(\"!@#$%^&*()_+[]{}|;':\\\",.<>?/~`\")\n","    UFV['url_special_characters'] = int(any(char in special_characters for char in url_str))\n","\n","    for tag in TG:\n","        UFV[f'url_tag_{tag}'] = int(tag in url_str)\n","\n","    for attribute in AT:\n","        UFV[f'url_attr_{attribute}'] = int(attribute in url_str)\n","\n","    for event in EV:\n","        UFV[f'url_event_{event}'] = int(event in url_str)\n","\n","\n","    UFV['url_redirection'] = 0\n","    UFV['url_number_keywords_param'] = 0\n","    UFV['url_number_keywords_evil'] = 0\n","\n","    UFV['url_redirection'] = int(any(param in url_str for param in url_redirections))\n","\n","    for param in url_number_keywords_param:\n","        UFV['url_number_keywords_param'] += int(param in url_str)\n","\n","    for keyword in url_number_keywords_evil:\n","        UFV['url_number_keywords_evil'] += int(keyword in url_str)\n","\n","    # UFV['url_cookie'] = int('document.Cookie' in url_str)\n","    cookie_pattern = re.compile(r'(document\\s*\\.\\s*cookie|document\\s*\\[\\s*\"cookie\"\\s*\\])', re.IGNORECASE)\n","\n","    UFV['url_cookie'] = int(bool(cookie_pattern.search(url_str)))\n","\n","    domains = re.findall(r'(?P<url>https?://\\S+)', url_str)\n","    UFV['url_number_domain'] = len(domains)\n","\n","    ips = re.findall(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url_str)\n","    UFV['url_number_ip'] = len(ips)\n","\n","    return UFV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSfouAqs_qHp"},"outputs":[],"source":["def parse_html(web_page):\n","    return BeautifulSoup(web_page, 'html5lib')\n","\n","def create_js_feature_vector(page_content):\n","\n","    DO = [\"document\", \"window\", \"navigator\", \"location\", \"localStorage\", \"sessionStorage\", \"history\", \"console\", \"alert\", \"confirm\", \"prompt\"]\n","    JP = [\"cookie\", \"document\" , \"referrer\", \"innerHTML\", \"innerText\", \"textContent\", \"value\", \"href\", \"src\", \"classList\", \"getAttribute\", \"setAttribute\"]\n","    JM = [\"write\", \"getElementsByTagName\", \"getElementById\", \"alert\", \"eval\", \"fromCharCode\", \"prompt\", \"confirm\", \"fetch\"]\n","\n","    P = parse_html(page_content)\n","\n","    JS_strings = []\n","\n","\n","    for script_tag in P.find_all(\"script\"):\n","        if not script_tag.attrs.get(\"src\"):\n","            js = script_tag.string\n","            if js:\n","                JS_strings.append(js)\n","\n","\n","    for a_tag in P.find_all(\"a\", href=True):\n","        if a_tag.attrs[\"href\"].startswith(\"javascript:\"):\n","            js = a_tag.attrs[\"href\"][len(\"javascript:\"):]\n","            if js:\n","                JS_strings.append(js)\n","\n","\n","    for form_tag in P.find_all(\"form\", action=True):\n","        js = form_tag.attrs[\"action\"]\n","        if js:\n","            JS_strings.append(js)\n","\n","\n","    for iframe_tag in P.find_all(\"iframe\", src=True):\n","        js = iframe_tag.attrs[\"src\"]\n","        if js:\n","            JS_strings.append(js)\n","\n","\n","    for frame_tag in P.find_all(\"frame\", src=True):\n","        js = frame_tag.attrs[\"src\"]\n","        if js:\n","            JS_strings.append(js)\n","\n","\n","    all_tokens = []\n","    for js_code in JS_strings:\n","        tokens = esprima.tokenize(js_code)\n","        all_tokens.extend(tokens)\n","\n","\n","    JSFV = {}\n","\n","    for do in DO:\n","        JSFV[f'js_dom_{do}'] = 0\n","    for jp in JP:\n","        JSFV[f'js_prop_{jp}'] = 0\n","    for jm in JM:\n","        JSFV[f'js_method_{jm}'] = 0\n","\n","\n","    Stringlist = []\n","    for token in all_tokens:\n","        if token.type == 'Identifier':\n","            value = token.value\n","            if value in DO:\n","                JSFV[f'js_dom_{value}'] += 1\n","            elif value in JP:\n","                JSFV[f'js_prop_{value}'] += 1\n","            elif value in JM:\n","                JSFV[f'js_method_{value}'] += 1\n","        elif token.type == 'String':\n","            string_value = token.value\n","            Stringlist.append(string_value)\n","\n","\n","    if Stringlist:\n","        JSFV['js_min_length'] = min(len(s) for s in Stringlist)\n","        JSFV['js_max_length'] = max(len(s) for s in Stringlist)\n","    else:\n","        JSFV['js_min_length'] = 0\n","        JSFV['js_max_length'] = 0\n","\n","\n","    JSFV['html_length'] = len(page_content)\n","\n","    #define function\n","    JSFV['js_define_function'] = 0\n","    JSFV['js_function_calls'] = 0\n","    soup = BeautifulSoup(page_content, 'html.parser')\n","    script_tags = soup.find_all('script')\n","    function_definition_pattern = r'function\\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\\s*\\('\n","\n","\n","    for script_tag in script_tags:\n","        script_code = script_tag.get_text()\n","        function_definitions = re.findall(function_definition_pattern, script_code)\n","        JSFV['js_define_function'] += len(function_definitions)\n","\n","\n","    #call_function\n","    function_call_pattern = r'(\\w+)\\s*\\('\n","    function_call_counts = {}\n","\n","    for script_tag in script_tags:\n","        script_code = script_tag.get_text()\n","        function_calls = re.findall(function_call_pattern, script_code)\n","\n","        for function_name in function_calls:\n","            if function_name in function_call_counts:\n","                function_call_counts[function_name] += 1\n","            else:\n","                function_call_counts[function_name] = 1\n","\n","    for function_name, call_count in function_call_counts.items():\n","        JSFV['js_function_calls'] += call_count - 1\n","\n","    #js file\n","    js_file_pattern = re.compile(r'\\.js')\n","\n","    if js_file_pattern.search(page_content):\n","        JSFV['js_file'] = 1\n","    else:\n","        JSFV['js_file'] = 0\n","\n","    #js_pseudo_protocol\n","    strings_to_check = [\n","        '<img src=\"javaascript:',\n","        '<form aaction=\"javascript:',\n","        '<object adata=\"javascript:',\n","        '<button formaction=\"javascript:',\n","        '<video src=\"javascript:',\n","        '<a href=\"javascript:',\n","        '<iframe src=\"javascript:'\n","    ]\n","\n","    regex_pattern = '|'.join(re.escape(string) for string in strings_to_check)\n","\n","    matches = re.search(regex_pattern, page_content)\n","\n","    if matches:\n","        JSFV['js_pseudo_protocol'] = 1\n","    else:\n","        JSFV['js_pseudo_protocol'] = 0\n","\n","    return JSFV"]},{"cell_type":"code","source":["# Crawl page and process HTML\n","crawled_data = []\n","data_html = []\n","data_js = []\n","data_url = []\n","def add_data(list1, list2):\n","    for dict in list2:\n","      list1.append(dict)\n","def crawl(url):\n","    try:\n","        if not urlparse.urlparse(url).scheme:\n","            url = \"http://\" + url\n","\n","        with requests.get(url, timeout=10) as r:\n","            html_content = r.text\n","\n","        page_url = url\n","\n","        url_feature_vector = parse_url_address(page_url)\n","        html_feature_vectors = create_html_feature_vector(html_content)\n","        js_feature_vector = create_js_feature_vector(html_content)\n","\n","        data_html.append(html_feature_vectors)\n","        data_url.append(url_feature_vector)\n","        data_js.append(js_feature_vector)\n","\n","\n","    except Exception as e:\n","        pass\n","\n","# Save page data (if needed)\n","def save_page(url, html_content, js_content):\n","    with open('crawl.csv', 'a') as f:\n","        writer = csv.writer(f)\n","        writer.writerow([url, html_content, js_content])\n","\n","def save_crawled_html_data_to_csv(filename, data):\n","    with open('html_crawl_xxs.csv', mode='w', newline='') as csv_file:\n","          fieldnames = data[0].keys()\n","          writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","\n","          writer.writeheader()\n","          writer.writerows(data)\n","def save_crawled_js_data_to_csv(filename, data):\n","    with open('js_crawl_xxs.csv', mode='w', newline='') as csv_file:\n","          fieldnames = data[0].keys()\n","          writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","\n","          writer.writeheader()\n","          writer.writerows(data)\n","def save_crawled_url_data_to_csv(filename, data):\n","    with open('url_crawl_xxs.csv', mode='w', newline='') as csv_file:\n","          fieldnames = data[0].keys()\n","          writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","\n","          writer.writeheader()\n","          writer.writerows(data)\n","def save_crawled_data_to_csv(filename, data, fields_name):\n","    with open('crawl_xxs.csv', mode='w', newline='') as csv_file:\n","          fieldnames = fields_name\n","          writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","\n","          writer.writeheader()\n","          writer.writerows(data)"],"metadata":{"id":"-jEgoKUfeiIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import re\n","import html\n","\n","def check_and_extract_url(url,i):\n","    try:\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            content = response.text\n","\n","            key_word_1 = \"UNFIXED\"\n","            key_word_2 = \"Category: XSS\"\n","            if key_word_1 in content and key_word_2 in content:\n","                extracted_url = extract_url_from_html(content)\n","                if extracted_url:\n","                    crawl(extracted_url)\n","\n","    except Exception as e:\n","        pass\n","\n","\n","def extract_url_from_html(html_code):\n","    decoded_html = html.unescape(html_code)\n","\n","    #print(decoded_html)\n","    url_pattern = r'URL:\\s(https?://[^\\n]+)' #r'URL:\\s(https?://[^\\s]+)'\n","    match = re.search(url_pattern, decoded_html)\n","\n","    if match:\n","        url = match.group(1).split(\"</th>\")[0].replace('<br>', '')\n","        return url\n","    else:\n","        return None\n"],"metadata":{"id":"ngVcsRIKgNZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0,30):\n","  print(f\"{i}\")\n","  url = f\"http://xssed.com/mirror/{str(i)}/\"\n","  check_and_extract_url(url,i)\n","\n","df_html = pd.DataFrame(data_html)\n","df_url = pd.DataFrame(data_url)\n","df_js = pd.DataFrame(data_js)\n","crawled_data = pd.concat([df_html, df_js, df_url], axis=1)\n","crawled_data['label'] = np.ones(crawled_data.shape[0])\n","\n","crawled_data.to_csv('/content/drive/MyDrive/crawl_xss.csv', index=False)\n","\n","from google.colab import files\n","files.download('/content/drive/MyDrive/crawl_xss.csv')"],"metadata":{"id":"aVE9Kxq_hC7g"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1_hVq0N_OHiiwUmVlF3NftG74kWoI3nrw","timestamp":1696177360124},{"file_id":"1Yu6YT5WYDDsQs5onXW3pzxe3DK1JCJC6","timestamp":1695303191831},{"file_id":"1W4-p1_41SXe3q_i1wMIuPxPDFySqbg2g","timestamp":1694437813409},{"file_id":"1wOQfdROEKFoF0rAnQbEFLNz2Sj6fw7UE","timestamp":1694104293137}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}